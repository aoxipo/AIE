{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07b48aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:35:52.429582Z",
     "start_time": "2023-11-30T07:35:48.766499Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b1be8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:35:53.377103Z",
     "start_time": "2023-11-30T07:35:52.431577Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34 as resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd15e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:35:53.945902Z",
     "start_time": "2023-11-30T07:35:53.379100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DentaLink\\.conda\\envs\\tooth\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.pvtv2 import pvt_v2_b2\n",
    "from model.model import BaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89865c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:35:53.961293Z",
     "start_time": "2023-11-30T07:35:53.947899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.Impression_classifier = nn.Sequential(nn.Linear(middle_channel[0], self.Impression))\n",
      "self.HyperF_Type_classifier = nn.Sequential(nn.Linear(middle_channel[1], self.HyperF_Type))\n",
      "self.HyperF_Area_classifier = nn.Sequential(nn.Linear(middle_channel[2], self.HyperF_Area))\n",
      "self.HyperF_Fovea_classifier = nn.Sequential(nn.Linear(middle_channel[3], self.HyperF_Fovea))\n",
      "self.HyperF_ExtraFovea_classifier = nn.Sequential(nn.Linear(middle_channel[4], self.HyperF_ExtraFovea))\n",
      "self.HyperF_Y_classifier = nn.Sequential(nn.Linear(middle_channel[5], self.HyperF_Y))\n",
      "self.HypoF_Type_classifier = nn.Sequential(nn.Linear(middle_channel[6], self.HypoF_Type))\n",
      "self.HypoF_Area_classifier = nn.Sequential(nn.Linear(middle_channel[7], self.HypoF_Area))\n",
      "self.HypoF_Fovea_classifier = nn.Sequential(nn.Linear(middle_channel[8], self.HypoF_Fovea))\n",
      "self.HypoF_ExtraFovea_classifier = nn.Sequential(nn.Linear(middle_channel[9], self.HypoF_ExtraFovea))\n",
      "self.HypoF_Y_classifier = nn.Sequential(nn.Linear(middle_channel[10], self.HypoF_Y))\n",
      "self.CNV_classifier = nn.Sequential(nn.Linear(middle_channel[11], self.CNV))\n",
      "self.Vascular_abnormality_classifier = nn.Sequential(nn.Linear(middle_channel[12], self.Vascular_abnormality))\n",
      "self.Pattern_classifier = nn.Sequential(nn.Linear(middle_channel[13], self.Pattern))\n",
      "return  Impression_res, HyperF_Type_res, HyperF_Area_res, HyperF_Fovea_res, HyperF_ExtraFovea_res, HyperF_Y_res, HypoF_Type_res, HypoF_Area_res, HypoF_Fovea_res, HypoF_ExtraFovea_res, HypoF_Y_res, CNV_res, Vascular_abnormality_res, Pattern_res,\n"
     ]
    }
   ],
   "source": [
    "data_key = [ \"Impression\", \"HyperF_Type\", \"HyperF_Area\", \"HyperF_Fovea\", \"HyperF_ExtraFovea\", \"HyperF_Y\", \n",
    "      \"HypoF_Type\" ,\"HypoF_Area\",\"HypoF_Fovea\", \"HypoF_ExtraFovea\"\n",
    "    ,\"HypoF_Y\",\"CNV\",\"Vascular_abnormality\",\"Pattern\"]\n",
    "ans = \"return \"\n",
    "for index in range(len(data_key)):\n",
    "    print(f\"self.{data_key[index]}_classifier = nn.Sequential(nn.Linear(middle_channel[{index}], self.{data_key[index]}))\")\n",
    "    # print(f\"{data_key[index]}_res = self.{data_key[index]}_classifier(x[{index}])\")\n",
    "    ans += f\" {data_key[index]}_res,\"\n",
    "print(ans ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36450753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5d02e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:35:53.992214Z",
     "start_time": "2023-11-30T07:35:53.965285Z"
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \n",
    "    # 类别数量\n",
    "    Impression = 23 # 0\n",
    "    HyperF_Type = 5 # 1\n",
    "    HyperF_Area = 3 # 2\n",
    "    HyperF_Fovea = 2 # 3\n",
    "    HyperF_ExtraFovea = 18 # 4\n",
    "    HyperF_Y = 4 #           5  \n",
    "    HypoF_Type = 3 #         6 \n",
    "    HypoF_Area = 3 #         7\n",
    "    HypoF_Fovea = 2 #        8\n",
    "    HypoF_ExtraFovea = 17 #  9\n",
    "    HypoF_Y = 5           #  10\n",
    "    CNV = 2               #  11\n",
    "    Vascular_abnormality = 15 # 12\n",
    "    Pattern = 14              # 13\n",
    "    # [0,1,6,7,9,10,11,13]\n",
    "    # [2,3,4,5,8,12]\n",
    "    def __init__(self, middle_channel):\n",
    "        super().__init__()\n",
    "        self.Impression_classifier = nn.Sequential(nn.Linear(middle_channel[0], self.Impression))\n",
    "        self.HyperF_Type_classifier = nn.Sequential(nn.Linear(middle_channel[1], self.HyperF_Type))\n",
    "        self.HyperF_Area_classifier = nn.Sequential(nn.Linear(middle_channel[2], self.HyperF_Area))\n",
    "        self.HyperF_Fovea_classifier = nn.Sequential(nn.Linear(middle_channel[3], self.HyperF_Fovea))\n",
    "        self.HyperF_ExtraFovea_classifier = nn.Sequential(nn.Linear(middle_channel[4], self.HyperF_ExtraFovea))\n",
    "        self.HyperF_Y_classifier = nn.Sequential(nn.Linear(middle_channel[5], self.HyperF_Y))\n",
    "        self.HypoF_Type_classifier = nn.Sequential(nn.Linear(middle_channel[6], self.HypoF_Type))\n",
    "        self.HypoF_Area_classifier = nn.Sequential(nn.Linear(middle_channel[7], self.HypoF_Area))\n",
    "        self.HypoF_Fovea_classifier = nn.Sequential(nn.Linear(middle_channel[8], self.HypoF_Fovea))\n",
    "        self.HypoF_ExtraFovea_classifier = nn.Sequential(nn.Linear(middle_channel[9], self.HypoF_ExtraFovea))\n",
    "        self.HypoF_Y_classifier = nn.Sequential(nn.Linear(middle_channel[10], self.HypoF_Y))\n",
    "        self.CNV_classifier = nn.Sequential(nn.Linear(middle_channel[11], self.CNV))\n",
    "        self.Vascular_abnormality_classifier = nn.Sequential(nn.Linear(middle_channel[12], self.Vascular_abnormality))\n",
    "        self.Pattern_classifier = nn.Sequential(nn.Linear(middle_channel[13], self.Pattern))\n",
    "    def forward(self, x):\n",
    "#         x,x2 = x\n",
    "        Impression_res = self.Impression_classifier(x[0])\n",
    "        HyperF_Type_res = self.HyperF_Type_classifier(x[1])\n",
    "        HyperF_Area_res = self.HyperF_Area_classifier(x[2])\n",
    "        HyperF_Fovea_res = self.HyperF_Fovea_classifier(x[3])\n",
    "        HyperF_ExtraFovea_res = self.HyperF_ExtraFovea_classifier(x[4])\n",
    "        HyperF_Y_res = self.HyperF_Y_classifier(x[5])\n",
    "        HypoF_Type_res = self.HypoF_Type_classifier(x[6])\n",
    "        HypoF_Area_res = self.HypoF_Area_classifier(x[7])\n",
    "        HypoF_Fovea_res = self.HypoF_Fovea_classifier(x[8])\n",
    "        HypoF_ExtraFovea_res = self.HypoF_ExtraFovea_classifier(x[9])\n",
    "        HypoF_Y_res = self.HypoF_Y_classifier(x[10])\n",
    "        CNV_res = self.CNV_classifier(x[11])\n",
    "        Vascular_abnormality_res = self.Vascular_abnormality_classifier(x[12])\n",
    "        Pattern_res = self.Pattern_classifier(x[13])\n",
    "        return  [\n",
    "            Impression_res, HyperF_Type_res, HyperF_Area_res, HyperF_Fovea_res, HyperF_ExtraFovea_res, \n",
    "            HyperF_Y_res, HypoF_Type_res, HypoF_Area_res, HypoF_Fovea_res, \n",
    "            HypoF_ExtraFovea_res, HypoF_Y_res, CNV_res, Vascular_abnormality_res, \n",
    "            Pattern_res \n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9928ba2",
   "metadata": {},
   "source": [
    "# 特征对齐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84116e90",
   "metadata": {},
   "source": [
    "a * b + c * d -> (a + c ) * (b + d) -> a * b + c * d + c * b + a * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "249248b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:47:08.422174Z",
     "start_time": "2023-11-30T07:47:08.391508Z"
    }
   },
   "outputs": [],
   "source": [
    "class AFC(nn.Module):            \n",
    "    def __init__(self, features_list, out_features, r = 2, L=32):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            features: input channel dimensionality.\n",
    "            r: the radio for compute d, the length of z.                 2      \n",
    "            L: the minimum dim of the vector z in paper, default 32\n",
    "        \"\"\"\n",
    "        super(AFC, self).__init__()\n",
    "        features = out_features\n",
    "        d = max(int(features/r), L)\n",
    "        self.M = len(features_list)\n",
    "        self.features = features\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(features, d)\n",
    "        self.fcs = nn.ModuleList()\n",
    "        for i in range(self.M):\n",
    "            self.fcs.append(\n",
    "                nn.Linear(d, features)\n",
    "            )\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "        \n",
    "    def normal(self, x):\n",
    "        return (x - x.min())/(x.max() - x.min())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        total = len(x) \n",
    "        for i in range(total):\n",
    "            fea = x[i].unsqueeze_(dim = 0).unsqueeze_(dim = 1)\n",
    "            # print(fea.shape)\n",
    "            if i == 0:\n",
    "                feas = fea\n",
    "            else:\n",
    "                feas = torch.cat([feas, fea], dim=1)\n",
    "        #         print(feas.shape)\n",
    "        fea_U = torch.sum(feas, dim=1)#.unsqueeze(0)\n",
    "        #         print(\"feau:\", fea_U.shape)\n",
    "        fea_s = self.gap(fea_U).squeeze(dim=-1).squeeze(dim=-1)\n",
    "        #         print(fea_s.shape)\n",
    "        fea_z = self.fc(fea_s)\n",
    "        #         print( fea_z.shape )\n",
    "        for i, fc in enumerate(self.fcs):\n",
    "            vector = fc(fea_z).unsqueeze_(dim=1)\n",
    "            if i == 0:\n",
    "                attention_vectors = vector\n",
    "            else:\n",
    "                attention_vectors = torch.cat([attention_vectors, vector], dim=1)\n",
    "           \n",
    "        attention_vectors = self.softmax(attention_vectors)\n",
    "        attention_vectors = attention_vectors.unsqueeze(-1).unsqueeze(-1)\n",
    "        fea_v = self.normal(feas.sum(dim=1)) * self.normal(attention_vectors.sum(dim=1).squeeze(1))\n",
    "        return fea_v\n",
    "\n",
    "class SKConv(nn.Module):\n",
    "    #                  64       32   2  8  2\n",
    "    def __init__(self, features_list, out_features, WH, M, G, r, stride=1 ,L=32):\n",
    "        \"\"\" Constructor\n",
    "        Args:\n",
    "            features: input channel dimensionality.\n",
    "            WH: input spatial dimensionality, used for GAP kernel size.\n",
    "            M: the number of branchs.\n",
    "            G: num of convolution groups.\n",
    "            r: the radio for compute d, the length of z.\n",
    "            stride: stride, default 1.\n",
    "            L: the minimum dim of the vector z in paper, default 32\n",
    "        \"\"\"\n",
    "        super(SKConv, self).__init__()\n",
    "        features = features_list[-1]\n",
    "        d = max(int(features/r), L)\n",
    "        self.M = M\n",
    "        self.features = features\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(M):\n",
    "            self.convs.append(nn.Sequential(\n",
    "                nn.Conv2d(features_list[i], out_features, kernel_size=3, stride=1, padding=1, groups=G),\n",
    "                nn.BatchNorm2d(out_features),\n",
    "                # nn.ReLU()\n",
    "            ))\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(features, d)\n",
    "        self.fcs = nn.ModuleList()\n",
    "        for i in range(M):\n",
    "            self.fcs.append(\n",
    "                nn.Linear(d, features)\n",
    "            )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            fea = conv(x[i]).unsqueeze_(dim=1)\n",
    "            if i == 0:\n",
    "                feas = fea\n",
    "            else:\n",
    "                feas = torch.cat([feas, fea], dim=1)\n",
    "            # print(i)\n",
    "        #print(feas.shape)\n",
    "        fea_U = torch.sum(feas, dim=1)\n",
    "        # print(fea_U.shape)\n",
    "        fea_s = self.gap(fea_U).squeeze(dim= -1).squeeze(dim= -1)\n",
    "        # print(fea_U.shape, fea_s.shape)\n",
    "        fea_z = self.fc(fea_s)\n",
    "        for i, fc in enumerate(self.fcs):\n",
    "            vector = fc(fea_z).unsqueeze_(dim=1)\n",
    "            if i == 0:\n",
    "                attention_vectors = vector\n",
    "            else:\n",
    "                attention_vectors = torch.cat([attention_vectors, vector], dim=1)\n",
    "            # print(i)\n",
    "        attention_vectors = self.softmax(attention_vectors)\n",
    "        attention_vectors = attention_vectors.unsqueeze(-1).unsqueeze(-1)\n",
    "        # if attention_vectors.shape[0] != 1 :\n",
    "        #     attention_vectors = attention_vectors.unsqueeze(0)\n",
    "        # print( attention_vectors.shape , feas.shape )\n",
    "        fea_v = (feas * attention_vectors).sum(dim=1)\n",
    "        return fea_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55d3ee64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:47:09.399597Z",
     "start_time": "2023-11-30T07:47:09.382445Z"
    }
   },
   "outputs": [],
   "source": [
    "a = AFC([32, 32,32,32], 32)\n",
    "sk = AFC([32, 32,32,32], 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54a7ed8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:44:44.715386Z",
     "start_time": "2023-11-30T07:44:44.697435Z"
    }
   },
   "outputs": [],
   "source": [
    "feature = torch.zeros((8, 32, 64, 64))\n",
    "feature2 = [ torch.zeros((2, 32, 64, 64))  for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fe476d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:44:53.345802Z",
     "start_time": "2023-11-30T07:44:53.311894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 32, 64, 64])\n",
      "torch.Size([2, 1, 32, 64, 64])\n",
      "torch.Size([2, 1, 32, 64, 64])\n",
      "torch.Size([2, 1, 32, 64, 64])\n",
      "torch.Size([2, 4, 32, 64, 64])\n",
      "feau: torch.Size([2, 32, 64, 64])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 64, 64])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk(feature2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e1ee969",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:47:10.801237Z",
     "start_time": "2023-11-30T07:47:10.768324Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 64, 64])\n",
      "torch.Size([1, 1, 32, 64, 64])\n",
      "torch.Size([1, 1, 32, 64, 64])\n",
      "torch.Size([1, 1, 32, 64, 64])\n",
      "torch.Size([1, 1, 32, 64, 64])\n",
      "torch.Size([1, 1, 32, 64, 64])\n",
      "torch.Size([1, 1, 32, 64, 64])\n",
      "torch.Size([1, 1, 32, 64, 64])\n",
      "torch.Size([1, 8, 32, 64, 64])\n",
      "feau: torch.Size([1, 32, 64, 64])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 64, 64])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(feature).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c3e2392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:41:24.315118Z",
     "start_time": "2023-11-30T07:41:24.304639Z"
    }
   },
   "outputs": [],
   "source": [
    "feature = [torch.zeros(1,64,24,24 ) for i in range(16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62b13d3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T07:41:24.764588Z",
     "start_time": "2023-11-30T07:41:24.648899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 16, 64, 24, 24])\n",
      "feau: torch.Size([1, 1, 64, 24, 24])\n",
      "torch.Size([1, 1, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x64 and 32x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\.conda\\envs\\tooth\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tooth\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 41\u001b[0m, in \u001b[0;36mAFC.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m fea_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgap(fea_U)\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(fea_s\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 41\u001b[0m fea_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfea_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m( fea_z\u001b[38;5;241m.\u001b[39mshape )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, fc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcs):\n",
      "File \u001b[1;32m~\\.conda\\envs\\tooth\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tooth\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tooth\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x64 and 32x32)"
     ]
    }
   ],
   "source": [
    "a(feature).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "222668f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T01:43:53.855346Z",
     "start_time": "2023-11-29T01:43:53.840387Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [[torch.zeros(32,3,32,32),torch.zeros(32,15,16,16),torch.zeros(32,24,12,12) ] for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36795d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T01:50:26.557056Z",
     "start_time": "2023-11-29T01:50:26.551082Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [[],[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89b2504f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T01:50:27.433107Z",
     "start_time": "2023-11-29T01:50:27.416398Z"
    }
   },
   "outputs": [],
   "source": [
    "a[0].append(1)\n",
    "a[1].append(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79437c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T01:50:29.657559Z",
     "start_time": "2023-11-29T01:50:29.648584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [32], [], []]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9e96f",
   "metadata": {},
   "source": [
    "backbone # ML 随机森林 森林 boast svm # neck head DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df69f97",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e10be6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T09:37:19.361814Z",
     "start_time": "2023-11-20T09:37:19.349479Z"
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "class CBLK(nn.Module):\n",
    "    def __init__(self, inc, ouc, k = 3, s = 1, p = 1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(inc, ouc, k, s, p),\n",
    "            nn.BatchNorm2d(ouc),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "# sppf  atn\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, inc, ouc):\n",
    "        super().__init__()\n",
    "        \n",
    "        d = ouc//4\n",
    "        print(\"inc {}\".format(inc))\n",
    "        self.c1 = nn.Sequential(\n",
    "            CBLK(inc[0], d),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.c2 = nn.Sequential(\n",
    "            CBLK(inc[1], d)\n",
    "        )\n",
    "        self.c3 = nn.Sequential(\n",
    "            CBLK(inc[2], d)\n",
    "        )\n",
    "        self.c4 = nn.Sequential(\n",
    "            CBLK(inc[3], d)\n",
    "        )\n",
    "        self.proj_k = nn.Conv2d(4*d, 4*d, 1,1 )\n",
    "        self.out = nn.Conv2d(4*d, ouc, 1,1 )\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.down = nn.MaxPool2d(2)\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        c1 = self.c1(x1)\n",
    "        c2 = self.c2(x2)\n",
    "        c3 = self.c3(x3)\n",
    "        c4 = self.c4(x4)\n",
    "        \n",
    "        c21 = torch.cat([c1,c2], 1)\n",
    "        print(\"c3 {} self.down(c21) {} \".format(c3.size(),self.down(c21).size()))\n",
    "        \n",
    "        print(\"c1 {} c2 {} c3 {} c4 {} \".format(c1.size(),c2.size(),c3.size(),c4.size() ))\n",
    "#      c3 torch.Size([2, 36, 14, 14]) self.down(c21) torch.Size([2, 72, 14, 14]) \n",
    "# c1 torch.Size([2, 36, 28, 28]) c2 torch.Size([2, 36, 28, 28]) c3 torch.Size([2, 36, 14, 14]) c4 torch.Size([2, 36, 7, 7])    \n",
    "\n",
    "        c321 = torch.cat([self.down(c21), c3],dim=1)\n",
    "        \n",
    "        v = torch.cat([self.down(c321), c4],dim=1)\n",
    "        k = self.proj_k(v)\n",
    "        q = self.softmax(torch.sum((self.down(self.down(c1 + c2) + c3) + c4), dim=1, keepdim=True))\n",
    "        \n",
    "        x = v @ self.drop(k)\n",
    "        feature = x @ q\n",
    "        out = self.out(feature)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fce62b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T09:37:19.376559Z",
     "start_time": "2023-11-20T09:37:19.363809Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Neck(nn.Module): \n",
    "    def __init__(self, pvt_decode, resnet_decode, num_conv_layers = [ 8, 6]):\n",
    "        super(Neck, self).__init__()\n",
    "        self.pvt_decode = pvt_decode\n",
    "        self.resnet_decode = resnet_decode\n",
    "        middle_channel = 144\n",
    "        last_channel = 32\n",
    "        middle_afc = 40\n",
    "        print(\"inc {}\".format(np.array(pvt_decode) + np.array(resnet_decode)))\n",
    "        self.focus = Fusion( np.array(pvt_decode) + np.array(resnet_decode), middle_channel)\n",
    "      \n",
    "        d = [ CBLK(middle_channel, middle_channel,1,1,0) for i in range(num_conv_layers[0]) ]\n",
    "        d.append(CBLK(middle_channel, last_channel ,1,1,0))\n",
    "        self.conv_layers1 = nn.Sequential(*d)\n",
    "        \n",
    "        d = [ CBLK(middle_channel, middle_channel,1,1,0) for i in range(num_conv_layers[1]) ]\n",
    "        d.append(CBLK(middle_channel, last_channel ,1,1,0))\n",
    "        self.conv_layers2 = nn.Sequential(*d)\n",
    "        self.conv = nn.ModuleList()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear=nn.ModuleList()\n",
    "        \n",
    "        for i in range(14):\n",
    "            self.linear.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(last_channel*7*7, 1024),#middle_channel*7*7, 1024),\n",
    "                    nn.Linear(1024, 768),\n",
    "                    nn.Linear(768, 512)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.afc_pvt_list = nn.ModuleList()\n",
    "        for i in pvt_decode:\n",
    "            self.afc_pvt_list.append(AFC([ i for j in range(middle_afc)], i, 8))\n",
    "        self.afc_pvt_list = nn.ModuleList()\n",
    "        for i in resnet_decode:\n",
    "            self.afc_res_list.append(AFC([ i for j in range(middle_afc)], i, 8))\n",
    "\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        fuse1 = torch.cat([x[0] , y[0]], 1)\n",
    "        fuse2 = torch.cat([x[1] , y[1]], 1)\n",
    "        fuse3 = torch.cat([x[2] , y[2]], 1)\n",
    "        fuse4 = torch.cat([x[3] , y[3]], 1)\n",
    "        # print(\"fuse1 {} fuse2 {} fuse3 {} fuse4 {} \".format(fuse1.size(),fuse2.size(),fuse3.size(),fuse4.size()))\n",
    "        combine_feature = self.focus( fuse1,fuse2,fuse3,fuse4 )\n",
    "        # print(\"combine feature:\", combine_feature.shape)\n",
    "        out1 = self.conv_layers1(combine_feature)\n",
    "        out2 = self.conv_layers2(combine_feature)\n",
    "        high = self.flatten(out1)\n",
    "        low = self.flatten(out2)\n",
    "        # print(high.shape, low.shape)\n",
    "        ans = []\n",
    "        for idx in range(14):\n",
    "            if idx < 8:\n",
    "                ans.append(self.linear[idx](high))\n",
    "            else:\n",
    "                ans.append(self.linear[idx](low))\n",
    "            \n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71dbbabf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T09:37:25.425722Z",
     "start_time": "2023-11-20T09:37:25.404432Z"
    },
    "code_folding": [
     23,
     33
    ]
   },
   "outputs": [],
   "source": [
    "class DUAL(BaseLine):\n",
    "    # 类别数量\n",
    "    Impression = 23 # 0\n",
    "    HyperF_Type = 5 # 1\n",
    "    HyperF_Area = 3 # 2\n",
    "    HyperF_Fovea = 2 # 3\n",
    "    HyperF_ExtraFovea = 18 # 4\n",
    "    HyperF_Y = 4 #           5  \n",
    "    HypoF_Type = 3 #         6 \n",
    "    HypoF_Area = 3 #         7\n",
    "    HypoF_Fovea = 2 #        8\n",
    "    HypoF_ExtraFovea = 17 #  9\n",
    "    HypoF_Y = 5           #  10\n",
    "    CNV = 2               #  11\n",
    "    Vascular_abnormality = 15 # 12\n",
    "    Pattern = 14              # 13\n",
    "    \n",
    "    # [0,1,6,7,9,10,11,13]\n",
    "    \n",
    "    # [2,3,4,5,8,12]\n",
    "    def __init__(self, neck_num = [8,6]) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # PVT 提取特征\n",
    "        # path = './pretrained_pth/pvt_v2_b2.pth' # 找我要\n",
    "        self.backbone = pvt_v2_b2()  # [64, 128, 320, 512]\n",
    "        #  save_model = torch.load(path)\n",
    "        #         model_dict = self.backbone.state_dict()\n",
    "        #         state_dict = {k: v for k, v in save_model.items() if k in model_dict.keys()}\n",
    "        #         model_dict.update(state_dict)\n",
    "        #         self.backbone.load_state_dict(model_dict)\n",
    "        #         n_p = sum(x.numel() for x in self.backbone.parameters()) # number parameters\n",
    "        #         n_g = sum(x.numel() for x in self.backbone.parameters() if x.requires_grad)  # number gradients\n",
    "        #         print(f\"pvt Summary: {len(list(self.backbone.modules()))} layers, {n_p} parameters, {n_p/1e6} M, {n_g} gradients\")\n",
    "        # RESNET 特征提取\n",
    "        self.resnet = resnet(pretrained=True) \n",
    "        # self.resnet.load_state_dict(torch.load('pretrained_pth/resnet34-43635321.pth')) # 找我要\n",
    "        n_p = sum(x.numel() for x in self.resnet.parameters()) # number parameters\n",
    "        n_g = sum(x.numel() for x in self.resnet.parameters() if x.requires_grad)  # number gradients\n",
    "        print(f\"ResNet Summary: {len(list(self.resnet.modules()))} layers, {n_p} parameters, {n_p/1e6} M, {n_g} gradients\")\n",
    "#         self.neck = Neck( pvt_feature = [64, 128, 320, 512], resnet_feature = [64, 128, 256, 512], num_conv_layers = neck_num)\n",
    "        self.neck = Neck( [64, 128, 320, 512],  [64, 128, 256, 512], num_conv_layers = neck_num)\n",
    "        self.head = Head(middle_channel=[512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512])\n",
    "\n",
    "    def pvt_backbone(self, x):\n",
    "        pvt_x = x.clone().detach()\n",
    "        if x.shape[1] == 1:\n",
    "            pvt_x = torch.cat([pvt_x,pvt_x,pvt_x], 1)\n",
    "\n",
    "        pvt = self.backbone(pvt_x)\n",
    "        # pvt_decode: x1:torch.Size([2, 64, 56, 56]), c2:torch.Size([2, 128, 28, 28]), c3:torch.Size([2, 320, 14, 14]), c4:torch.Size([2, 512, 7, 7])\n",
    "        \n",
    "        return pvt\n",
    "\n",
    "    def resnet_backbone(self, x):\n",
    "        # if x.shape[1] == 1:\n",
    "        #     x = torch.cat([x,x,x], 1)\n",
    "        x   = self.resnet.conv1(x)\n",
    "        x   = self.resnet.bn1(x)\n",
    "        x   = self.resnet.relu(x)\n",
    "        \n",
    "        # - low-level features\n",
    "        x0  = self.resnet.maxpool(x)       \n",
    "        x1  = self.resnet.layer1(x0)       \n",
    "        x2  = self.resnet.layer2(x1)       \n",
    "        x3  = self.resnet.layer3(x2)     \n",
    "        x4  = self.resnet.layer4(x3)     \n",
    "        #res: x1:torch.Size([2, 64, 56, 56]), c2:torch.Size([2, 128, 28, 28]), c3:torch.Size([2, 256, 14, 14]), c4:torch.Size([2, 512, 7, 7])\n",
    "        #print(f\"res:x:{x.shape}, x1:{x1.shape}, c2:{x2.shape}, c3:{x3.shape}, c4:{x4.shape}\")\n",
    "        # print(f\"pvt_decode:x:{x.shape}, x1:{pvt_decode[0].shape}, c2:{pvt_decode[1].shape}, c3:{pvt_decode[2].shape}, c4:{pvt_decode[3].shape}\")\n",
    "        return x1,x2, x3,x4\n",
    "    def forward(self, x):\n",
    "        pvt_decode = self.pvt_backbone(x)     \n",
    "        res_decode = self.resnet_backbone(x)\n",
    "        feature_neck = self.neck( pvt_decode, res_decode)\n",
    "        classifier = self.head(feature_neck)\n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b491c2cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T09:37:27.368415Z",
     "start_time": "2023-11-20T09:37:25.999586Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DentaLink\\.conda\\envs\\tooth\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DentaLink\\.conda\\envs\\tooth\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Summary: 116 layers, 21797672 parameters, 21.797672 M, 21797672 gradients\n",
      "inc [ 128  256  576 1024]\n",
      "inc [ 128  256  576 1024]\n"
     ]
    }
   ],
   "source": [
    "model = DUAL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9574c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "33553a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 7, 7])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.neck.conv_layers1( torch.zeros(2,144,7,7) ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc69ad1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuse1 torch.Size([2, 128, 56, 56]) fuse2 torch.Size([2, 256, 28, 28]) fuse3 torch.Size([2, 576, 14, 14]) fuse4 torch.Size([2, 1024, 7, 7]) \n",
      "c3 torch.Size([2, 36, 14, 14]) self.down(c21) torch.Size([2, 72, 14, 14]) \n",
      "c1 torch.Size([2, 36, 28, 28]) c2 torch.Size([2, 36, 28, 28]) c3 torch.Size([2, 36, 14, 14]) c4 torch.Size([2, 36, 7, 7]) \n",
      "torch.Size([2, 144, 7, 7])\n",
      "combine feature: torch.Size([2, 144, 7, 7])\n",
      "torch.Size([2, 1568]) torch.Size([2, 1568])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DentaLink\\.conda\\envs\\tooth\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "batch_image = torch.zeros((2,3,224,224))\n",
    "ans = model(batch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702d094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "496796fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Impression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37617633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value in tensor 0: 0.1530095934867859\n",
      "Max value in tensor 1: 0.18238547444343567\n",
      "Max value in tensor 2: 0.12089018523693085\n",
      "Max value in tensor 3: 0.06578119844198227\n",
      "Max value in tensor 4: 0.19104671478271484\n",
      "Max value in tensor 5: 0.07583684474229813\n",
      "Max value in tensor 6: 0.13103041052818298\n",
      "Max value in tensor 7: 0.1117761880159378\n",
      "Max value in tensor 8: 0.13914397358894348\n",
      "Max value in tensor 9: 0.15929335355758667\n",
      "Max value in tensor 10: 0.20197774469852448\n",
      "Max value in tensor 11: 0.10965007543563843\n",
      "Max value in tensor 12: 0.17611879110336304\n",
      "Max value in tensor 13: 0.14662733674049377\n"
     ]
    }
   ],
   "source": [
    "for i, tensor in enumerate(ans):\n",
    "    max_value = torch.max(tensor)\n",
    "    print(f\"Max value in tensor {i}: {max_value.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d98372a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*7*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56f19aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.897959183673468"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024/49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f91ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tooth",
   "language": "python",
   "name": "tooth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
