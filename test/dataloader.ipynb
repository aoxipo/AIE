{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e22b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0970b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r\"D:\\dataset\\eye\\Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb50bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = root_path + \"/Train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe2fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6d2b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = [ \"Impression\", \"HyperF_Type\", \"HyperF_Area(DA)\", \"HyperF_Fovea\", \"HyperF_ExtraFovea\", \"HyperF_Y\", \n",
    "      \"HypoF_Type\" ,\"HypoF_Area(DA)\",\"HypoF_Fovea\", \"HypoF_ExtraFovea\"\n",
    "    ,\"HypoF_Y\",\"CNV\",\"Vascular abnormality (DR)\",\"Pattern\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72c8efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONDITION:\n",
    "    def __init__(self, name, idx, belong_name, belong_idx):\n",
    "        self.name = name\n",
    "        self.idx = idx\n",
    "        self.belong_name = belong_name\n",
    "        self.belong_idx = belong_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "947f7599",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def anylaized(data_source):\n",
    "    data_key = [ \"Impression\", \"HyperF_Type\", \"HyperF_Area(DA)\", \"HyperF_Fovea\", \"HyperF_ExtraFovea\", \"HyperF_Y\", \n",
    "      \"HypoF_Type\" ,\"HypoF_Area(DA)\",\"HypoF_Fovea\", \"HypoF_ExtraFovea\"\n",
    "    ,\"HypoF_Y\",\"CNV\",\"Vascular abnormality (DR)\",\"Pattern\"]\n",
    "    discribe_word = {}\n",
    "    discribe_key_word = {}\n",
    "    discribe_key_index = {}\n",
    "    index = 0\n",
    "    \n",
    "    for keyword in data_key:\n",
    "        discribe_key_index[keyword] = index\n",
    "        index += 1\n",
    "        impress_type = {}\n",
    "        for i in data_source[keyword]:\n",
    "            #if i is None:\n",
    "            if i is np.nan:\n",
    "                i = 'nan'\n",
    "            word_list = i.split(',')\n",
    "            for word in word_list:\n",
    "                if word == '':\n",
    "                    continue\n",
    "                if word not in impress_type:\n",
    "                    impress_type[word] = 1\n",
    "                else:\n",
    "                    impress_type[word] += 1\n",
    "        discribe_key_word[keyword] = impress_type\n",
    "        for key, value in impress_type.items():\n",
    "            if key not in discribe_word:\n",
    "                discribe_word[key] = 1\n",
    "            else:\n",
    "                discribe_word[key] += 1\n",
    "    return discribe_word, discribe_key_word, discribe_key_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "85d516c6",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def encode_row(discribe_key_word, discribe_key_index, discribe_word, data_key):\n",
    "    index = 0\n",
    "    gt_row_name2index_dict = {}\n",
    "    for i in data_key:\n",
    "        gt_row_name2index_dict[i] = index\n",
    "        index += 1\n",
    "    gt_dict = {}\n",
    "    gt_index2name_dict = {}\n",
    "    key_index = 0\n",
    "    for key, value in discribe_key_word.items(): \n",
    "        gt_dict[key] = {}\n",
    "        gt_index2name_dict[gt_row_name2index_dict[key]] = {}\n",
    "        index = 0\n",
    "        for gt_key_name in value.keys():\n",
    "            gt_dict[key][gt_key_name] = index\n",
    "            gt_index2name_dict[gt_row_name2index_dict[key]][index]=gt_key_name \n",
    "            discribe_word[gt_key_name] = CONDITION( gt_key_name, index, key, discribe_key_index[key] )\n",
    "            # print(gt_key_name)\n",
    "            index += 1\n",
    "    return gt_dict, gt_index2name_dict, discribe_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9c116744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_index2name_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a40a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "discribe_word, discribe_key_word, discribe_key_index = anylaized(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db0c558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict, gt_index2name_dict, discribe_word = encode_row(discribe_key_word, discribe_key_index, discribe_word, data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f188ee8f",
   "metadata": {
    "code_folding": [
     33,
     44,
     67
    ]
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataLoad(Dataset):\n",
    "    def __init__(self, root_path = r\"D:\\dataset\\eye\\Train\" ) -> None:\n",
    "        self.root_path = root_path\n",
    "        csv_path = root_path + \"/Train.csv\"\n",
    "        self.data_source = pd.read_csv(csv_path)\n",
    "        data_key = [ \"Impression\", \"HyperF_Type\", \"HyperF_Area(DA)\", \"HyperF_Fovea\", \"HyperF_ExtraFovea\", \"HyperF_Y\", \n",
    "          \"HypoF_Type\" ,\"HypoF_Area(DA)\",\"HypoF_Fovea\", \"HypoF_ExtraFovea\"\n",
    "        ,\"HypoF_Y\",\"CNV\",\"Vascular abnormality (DR)\",\"Pattern\"]\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "        discribe_word, discribe_key_word, discribe_key_index = anylaized(self.data_source)\n",
    "        gt_dict, gt_index2name_dict, discribe_word = encode_row(discribe_key_word, discribe_key_index, discribe_word, data_key)\n",
    "        self.gt_dict = gt_dict\n",
    "        self.gt_index2name_dict = gt_index2name_dict\n",
    "        self.discribe_word = discribe_word\n",
    "        \n",
    "        index = 0\n",
    "        self.data_key2index = {}\n",
    "        self.data_key_len = {}\n",
    "        \n",
    "        for i in data_key:\n",
    "            self.data_key2index[i] = index\n",
    "            self.data_key_len[index] = len(self.gt_dict[i])\n",
    "            index+=1\n",
    "        \n",
    "        self.load_data(self.data_source)\n",
    "        self.one_hot_label_set()\n",
    "        \n",
    "    def check_word(self, discribe):\n",
    "        if discribe is np.nan:\n",
    "            discribe = 'nan'\n",
    "        word_list = discribe.split(',')\n",
    "        ans = []\n",
    "        for word in word_list:\n",
    "            if word == '':\n",
    "                continue\n",
    "            ans.append(word)\n",
    "        return ans\n",
    "    \n",
    "    def one_hot_label_set(self):\n",
    "        self.photo_set_one_hot = []\n",
    "        for obj in self.photo_set:\n",
    "            data_key_idx = 0\n",
    "            # [[1,3], [1], [2], [0], [0], [0], [0], [2], [0], [0], [0], [0], [0], [0]]\n",
    "            gt_data_key = []\n",
    "            for data_key_item_gt in obj['gt']:\n",
    "                total = self.data_key_len[data_key_idx]\n",
    "                label_one_hot = np.zeros(total)\n",
    "                \n",
    "                for msg in data_key_item_gt:\n",
    "                    label_one_hot[msg] = 1\n",
    "                gt_data_key.append( label_one_hot )\n",
    "                data_key_idx += 1 \n",
    "            \n",
    "            self.photo_set_one_hot.append({\n",
    "                'image':obj['image'],\n",
    "                'gt':obj['gt'],\n",
    "                'gt_oh':gt_data_key,\n",
    "            })\n",
    "            \n",
    "            \n",
    "    \n",
    "    def load_data(self, data_source):\n",
    "        photo_set = []\n",
    "        img_root_path = self.root_path + \"/Train/\"\n",
    "        for index, row in data_source.iterrows():\n",
    "            gt_index = []\n",
    "            for discribe_list in row[:-2]:\n",
    "                discribe_list = self.check_word(discribe_list)\n",
    "                for discribe in discribe_list:\n",
    "                    temp_gt_idx = []\n",
    "                    code = self.discribe_word[discribe].idx\n",
    "                    temp_gt_idx.append( code )\n",
    "                gt_index.append(temp_gt_idx)\n",
    "            file_folder = row[-1]\n",
    "            file_path = img_root_path + file_folder + \"/\"\n",
    "            if os.path.exists(file_path):\n",
    "                file_name_list = os.listdir(file_path)\n",
    "                for file_name in file_name_list:\n",
    "                    img_obj = {\n",
    "                        \"image\" : file_path  + file_name,\n",
    "                        \"gt\" : gt_index,\n",
    "                    }\n",
    "                    photo_set.append(img_obj)\n",
    "            else:\n",
    "                print(file_path, \"not exists!\"  )\n",
    "                \n",
    "        self.photo_set = photo_set\n",
    "        self.total_number = len(self.photo_set)\n",
    "        print(\"total load data:\", self.total_number)\n",
    "    \n",
    "    def set_gan(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.photo_set)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取对应index的图像,并视情况进行数据增强\n",
    "        \"\"\"\n",
    "        if index >= self.total_number:\n",
    "            raise StopIteration\n",
    "        try:\n",
    "            re_index = index\n",
    "            image_src_path, image_gt = self.photo_set_one_hot[re_index]['image'], self.photo_set_one_hot[re_index]['gt']\n",
    "            \n",
    "            #             if self.datagen is not None:\n",
    "            #                 image_src = self.datagan_read(image_src)\n",
    "            #                 seed = torch.random.seed()\n",
    "            #                 torch.random.manual_seed(seed)\n",
    "            #                 image_src = self.datagan_random(image_src)\n",
    "            #                 image_src = self.datagan_normal(image_src)\n",
    "\n",
    "\n",
    "            #             image_edge_mask = self.get_edge(image_mask)\n",
    "            #             if self.datagen_gt is not None:\n",
    "\n",
    "            #                 image_mask = self.datagan_read(image_mask)\n",
    "            #                 torch.random.manual_seed(seed)\n",
    "            #                 image_mask = self.datagan_random(image_mask)\n",
    "            #                 image_mask = self.datagan_normal(image_mask)\n",
    "\n",
    "\n",
    "\n",
    "            return image_src, image_mask #\n",
    "        except Exception as e:\n",
    "            print(\"发现异常\")\n",
    "            print(e.__class__.__name__)\n",
    "            print(e)\n",
    "            print(index)\n",
    "            print(traceback.print_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "36a19d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DentaLink\\AppData\\Local\\Temp\\ipykernel_16736\\4204614676.py:80: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  file_folder = row[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dataset\\eye\\Train/Train/454_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/454_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/611_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/611_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/729_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1412_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1412_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1417_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1417_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1425_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1425_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1426_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1426_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1427_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1427_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1429_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1429_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1430_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1430_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1431_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1431_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1432_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1434_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1434_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1435_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1435_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1436_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1436_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1439_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1439_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1441_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1441_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1442_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1442_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1443_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1443_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1444_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1445_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1447_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1447_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1452_L/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1452_R/ not exists!\n",
      "D:\\dataset\\eye\\Train/Train/1563_L/ not exists!\n",
      "total load data: 33559\n"
     ]
    }
   ],
   "source": [
    "a = DataLoad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5b1e09d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': 'D:\\\\dataset\\\\eye\\\\Train/Train/76_R/14.jpg',\n",
       " 'gt': [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [2]],\n",
       " 'gt_oh': [array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0.]),\n",
       "  array([1., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([1., 0., 0., 0.]),\n",
       "  array([1., 0., 0.]),\n",
       "  array([1., 0., 0.]),\n",
       "  array([1., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0.]),\n",
       "  array([1., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.photo_set_one_hot[1078]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4ad114dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.gt_dict['Impression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9c2eb4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Impression': {'macular neovascularization': 0,\n",
       "  'dry age-related macular degeneration': 1,\n",
       "  'cystoid macular edema': 2,\n",
       "  'unremarkable changes': 3,\n",
       "  'central serous chorioretinopathy': 4,\n",
       "  'pachychoroid pigment epitheliopathy': 5,\n",
       "  'other': 6,\n",
       "  'retinal pigment epithelial detachment': 7,\n",
       "  'uveitis': 8,\n",
       "  'chorioretinal scar': 9,\n",
       "  'choroidal mass': 10,\n",
       "  'diabetic retinopathy': 11,\n",
       "  'myopia': 12,\n",
       "  'chorioretinal atrophy': 13,\n",
       "  'branch retinal vein occlusion': 14,\n",
       "  'retinal vein occlusion': 15,\n",
       "  'epiretinal membrane': 16,\n",
       "  'proliferative diabetic retinopathy': 17,\n",
       "  'central retinal vein occlusion': 18,\n",
       "  'central retinal artery occlusion': 19,\n",
       "  'polypoidal choroidal vasculopathy': 20,\n",
       "  'retinal arterial macroaneurysm': 21,\n",
       "  'retinal dystrophy': 22},\n",
       " 'HyperF_Type': {'leakage': 0,\n",
       "  'staining': 1,\n",
       "  'no': 2,\n",
       "  'pooling': 3,\n",
       "  'window defect': 4},\n",
       " 'HyperF_Area(DA)': {'4': 0, '5': 1, 'no': 2},\n",
       " 'HyperF_Fovea': {'yes': 0, 'no': 1},\n",
       " 'HyperF_ExtraFovea': {'no': 0,\n",
       "  'nasal': 1,\n",
       "  'superior': 2,\n",
       "  'superior nasal': 3,\n",
       "  'temporal': 4,\n",
       "  'inferior nasal': 5,\n",
       "  'superior temporal': 6,\n",
       "  'inferior temporal': 7,\n",
       "  'inferior': 8,\n",
       "  'superotemporal': 9,\n",
       "  'diffuse': 10,\n",
       "  'disc': 11,\n",
       "  'inferior to disc': 12,\n",
       "  'superior to disc': 13,\n",
       "  'temporal to disc': 14,\n",
       "  'nasal to disc': 15,\n",
       "  'periphery': 16,\n",
       "  'venular': 17},\n",
       " 'HyperF_Y': {'subretinal': 0, 'intraretinal': 1, 'no': 2, 'preretinal': 3},\n",
       " 'HypoF_Type': {'blockage': 0, 'no': 1, 'capillary non-perfusion': 2},\n",
       " 'HypoF_Area(DA)': {'4': 0, 'no': 1, '5': 2},\n",
       " 'HypoF_Fovea': {'yes': 0, 'no': 1},\n",
       " 'HypoF_ExtraFovea': {'no': 0,\n",
       "  'inferior temporal': 1,\n",
       "  'superior nasal': 2,\n",
       "  'superior temporal': 3,\n",
       "  'temporal': 4,\n",
       "  'nasal': 5,\n",
       "  'inferior': 6,\n",
       "  'superior': 7,\n",
       "  'diffuse': 8,\n",
       "  'inferior nasal': 9,\n",
       "  'superior to disc': 10,\n",
       "  'temporal to disc': 11,\n",
       "  'disc': 12,\n",
       "  'inferior to disc': 13,\n",
       "  'nan': 14,\n",
       "  'nasal to disc': 15,\n",
       "  'periphery': 16},\n",
       " 'HypoF_Y': {'subretinal': 0,\n",
       "  'no': 1,\n",
       "  'preretinal': 2,\n",
       "  'intraretinal': 3,\n",
       "  'nan': 4},\n",
       " 'CNV': {'yes': 0, 'no': 1},\n",
       " 'Vascular abnormality (DR)': {'no': 0,\n",
       "  'tortuous': 1,\n",
       "  'microaneurysm': 2,\n",
       "  'intraretinal microvascular abnormalities': 3,\n",
       "  'venous beading': 4,\n",
       "  'retinal neovascularization elsewhere': 5,\n",
       "  'collateral vessel': 6,\n",
       "  'telangiectasia': 7,\n",
       "  'vasculitis': 8,\n",
       "  'retinal neovascularization of the disc': 9,\n",
       "  'optociliary shunt': 10,\n",
       "  'vessel dilation': 11,\n",
       "  'macroaneurysm': 12,\n",
       "  'mild tortuous vessel': 13,\n",
       "  'tortuous dilate': 14},\n",
       " 'Pattern': {'no': 0,\n",
       "  'pcv': 1,\n",
       "  'polyp': 2,\n",
       "  'petaloid': 3,\n",
       "  'ink blot': 4,\n",
       "  'smoke stack': 5,\n",
       "  'branching neovascular network': 6,\n",
       "  'panretinal photocoagulation': 7,\n",
       "  'laser scar': 8,\n",
       "  'retinal pigment epithelial tear': 9,\n",
       "  'light bulb': 10,\n",
       "  'starry sky': 11,\n",
       "  'segmental panretinal photocoagulation': 12,\n",
       "  'drusen': 13}}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4727f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for i in data_key:\n",
    "    a.gt_dict[i] = index\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2da4c20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dry age-related macular degeneration', 'staining', 'no')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gt_index2name_dict[0][1], a.gt_index2name_dict[1][1], a.gt_index2name_dict[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9f7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None,max_images =10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.max_images = max_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, -1])\n",
    "        image_count = len(os.listdir(folder_name))\n",
    "        print(\"folder {}\".format(folder_name))\n",
    "        \n",
    "        images = [Image.open(os.path.join(folder_name, f\"{i}.jpg\")) for i in range(image_count)]\n",
    "        print(\"images {}\".format(len(images)))\n",
    "        \n",
    "        # Normalize and convert images to tensor\n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "        \n",
    "        while len(images) < self.max_images:\n",
    "            zero_tensor = torch.zeros([3, 224, 224])  # Assuming the images are 224x224 and 3-channel after transformation\n",
    "            images.append(zero_tensor)\n",
    "        images = torch.stack(images)  # Assuming all images in folder are related and stacking them into a single tensor\n",
    "        \n",
    "        labels = self.data_frame.iloc[idx, :-2]  # Excluding ID and Folder columns\n",
    "        labels = torch.tensor(labels.values.astype('float32'))\n",
    "        \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f39a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca49ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transformation (Normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resizing to 224x224 for example\n",
    "    transforms.ToTensor(),  # Transform it to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizing\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48f1eac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder F:\\Train\\Train\\1578_L\n",
      "images 9\n",
      "folder F:\\Train\\Train\\745_L\n",
      "images 23\n",
      "folder F:\\Train\\Train\\512_R\n",
      "images 3\n",
      "folder F:\\Train\\Train\\187_L\n",
      "images 14\n",
      "Batch 1\n",
      "Images shape: torch.Size([4, 23, 3, 224, 224]), Labels shape: torch.Size([4, 14])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = CustomDataset(csv_file=csv_path, root_dir=root_path, transform=transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    \n",
    "    # Pad images\n",
    "    images = pad_sequence(images, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack labels\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# DataLoader with custom collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Display a sample batch\n",
    "for i_batch, (images, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {i_batch + 1}\")\n",
    "    print(f\"Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "    break  # Display only one batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4893e6e",
   "metadata": {},
   "source": [
    "##  处理csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668d87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "data_frame = pd.read_csv('F:/Train/Train/Train.csv')\n",
    "\n",
    "# 初始化LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# 创建一个新的DataFrame用于存储转换后的数据\n",
    "transformed_data_frame = data_frame.copy()\n",
    "\n",
    "# 对每一个标签列应用LabelEncoder\n",
    "for column in data_frame.columns[:-2]:  # 排除 ID 和 Folder 列\n",
    "    transformed_data_frame[column] = label_encoder.fit_transform(data_frame[column])\n",
    "\n",
    "# 保存转换后的DataFrame\n",
    "transformed_data_frame.to_csv('F:/Train/Transformed_Train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fa979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tooth",
   "language": "python",
   "name": "tooth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
