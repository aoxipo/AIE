{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b93c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b031a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r\"D:\\dataset\\eye\\Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bdd206",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = root_path + \"/Train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a71046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r\"F:\\Train\\Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78a78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = root_path + \"/Transformed_Train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ee2da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Train\\\\Train/Transformed_Train.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4572ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbce6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None,max_images =10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.max_images = max_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, -1])\n",
    "        image_count = len(os.listdir(folder_name))\n",
    "        print(\"folder {}\".format(folder_name))\n",
    "        \n",
    "        images = [Image.open(os.path.join(folder_name, f\"{i}.jpg\")) for i in range(image_count)]\n",
    "        print(\"images {}\".format(len(images)))\n",
    "        \n",
    "        # Normalize and convert images to tensor\n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "        \n",
    "        while len(images) < self.max_images:\n",
    "            zero_tensor = torch.zeros([3, 224, 224])  # Assuming the images are 224x224 and 3-channel after transformation\n",
    "            images.append(zero_tensor)\n",
    "        images = torch.stack(images)  # Assuming all images in folder are related and stacking them into a single tensor\n",
    "        \n",
    "        labels = self.data_frame.iloc[idx, :-2]  # Excluding ID and Folder columns\n",
    "        labels = torch.tensor(labels.values.astype('float32'))\n",
    "        \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ea7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transformation (Normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resizing to 224x224 for example\n",
    "    transforms.ToTensor(),  # Transform it to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizing\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e07bbd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder F:\\Train\\Train\\1578_L\n",
      "images 9\n",
      "folder F:\\Train\\Train\\745_L\n",
      "images 23\n",
      "folder F:\\Train\\Train\\512_R\n",
      "images 3\n",
      "folder F:\\Train\\Train\\187_L\n",
      "images 14\n",
      "Batch 1\n",
      "Images shape: torch.Size([4, 23, 3, 224, 224]), Labels shape: torch.Size([4, 14])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = CustomDataset(csv_file=csv_path, root_dir=root_path, transform=transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    \n",
    "    # Pad images\n",
    "    images = pad_sequence(images, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack labels\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# DataLoader with custom collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Display a sample batch\n",
    "for i_batch, (images, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {i_batch + 1}\")\n",
    "    print(f\"Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "    break  # Display only one batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ec8bb",
   "metadata": {},
   "source": [
    "##  处理csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509c638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "data_frame = pd.read_csv('F:/Train/Train/Train.csv')\n",
    "\n",
    "# 初始化LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# 创建一个新的DataFrame用于存储转换后的数据\n",
    "transformed_data_frame = data_frame.copy()\n",
    "\n",
    "# 对每一个标签列应用LabelEncoder\n",
    "for column in data_frame.columns[:-2]:  # 排除 ID 和 Folder 列\n",
    "    transformed_data_frame[column] = label_encoder.fit_transform(data_frame[column])\n",
    "\n",
    "# 保存转换后的DataFrame\n",
    "transformed_data_frame.to_csv('F:/Train/Transformed_Train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ad9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
