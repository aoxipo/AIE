{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d99e02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./PVT/classification/\")\n",
    "\n",
    "from pvt import pvt_medium \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Assume you have the PVT model implementation\n",
    "from pvt import pvt_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84bea14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\soft\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Administrator/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Assume each task has a different number of classes\u001b[39;00m\n\u001b[0;32m     39\u001b[0m num_classes_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m70\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m110\u001b[39m, \u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m130\u001b[39m, \u001b[38;5;241m140\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDualEncoderWithMultiTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m     42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_tensor)\n",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m, in \u001b[0;36mDualEncoderWithMultiTask.__init__\u001b[1;34m(self, num_classes_list, pretrained)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(DualEncoderWithMultiTask, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define the dual encoder as before\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mDualEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a classification head for each task\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_heads \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m     10\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_encoder\u001b[38;5;241m.\u001b[39mcombined_features_dim, out_features\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m num_classes \u001b[38;5;129;01min\u001b[39;00m num_classes_list\n\u001b[0;32m     12\u001b[0m ])\n",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m, in \u001b[0;36mDualEncoder.__init__\u001b[1;34m(self, pretrained)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28msuper\u001b[39m(DualEncoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet \u001b[38;5;241m=\u001b[39m \u001b[43mresnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpvt \u001b[38;5;241m=\u001b[39m pvt(pretrained\u001b[38;5;241m=\u001b[39mpretrained)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet\u001b[38;5;241m.\u001b[39mchildren())[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\resnet.py:726\u001b[0m, in \u001b[0;36mresnet50\u001b[1;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \n\u001b[0;32m    702\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    724\u001b[0m weights \u001b[38;5;241m=\u001b[39m ResNet50_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBottleneck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\resnet.py:301\u001b[0m, in \u001b[0;36m_resnet\u001b[1;34m(block, layers, weights, progress, **kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet(block, layers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_api.py:63\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[1;34m(self, progress)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, progress: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\hub.py:727\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[0;32m    725\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 727\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location)\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\hub.py:593\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    591\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    592\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.hub\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m--> 593\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m meta \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(meta, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetheaders\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\urllib\\request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    522\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    524\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 525\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    528\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\urllib\\request.py:542\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    541\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 542\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    543\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\urllib\\request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\urllib\\request.py:1397\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\urllib\\request.py:1354\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1353\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1354\u001b[0m         \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1356\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m   1357\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1254\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1251\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1009\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1011\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1014\u001b[0m \n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 951\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\http\\client.py:1425\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1423\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    496\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1040\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   1038\u001b[0m             \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1040\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mD:\\soft\\Anaconda\\envs\\pytorch\\lib\\ssl.py:1309\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1309\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DualEncoderWithMultiTask(nn.Module):\n",
    "    def __init__(self, num_classes_list, pretrained=True):\n",
    "        super(DualEncoderWithMultiTask, self).__init__()\n",
    "        \n",
    "        # Define the dual encoder as before\n",
    "        self.dual_encoder = DualEncoder(pretrained=pretrained)\n",
    "        \n",
    "        # Create a classification head for each task\n",
    "        self.classification_heads = nn.ModuleList([\n",
    "            nn.Linear(in_features=self.dual_encoder.combined_features_dim, out_features=num_classes)\n",
    "            for num_classes in num_classes_list\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features using the dual encoder\n",
    "        features = self.dual_encoder(x)\n",
    "        \n",
    "        # Apply each classification head to the features\n",
    "        outputs = [classification_head(features) for classification_head in self.classification_heads]\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.resnet = resnet50(pretrained=pretrained)\n",
    "        self.pvt = pvt(pretrained=pretrained)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n",
    "        self.pvt = nn.Sequential(*list(self.pvt.children())[:-2])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        resnet_features = self.resnet(x)\n",
    "        pvt_features = self.pvt(x)\n",
    "        combined_features = torch.cat((resnet_features, pvt_features), dim=1)\n",
    "        return combined_features\n",
    "\n",
    "# Example usage:\n",
    "# Assume each task has a different number of classes\n",
    "num_classes_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140]\n",
    "model = DualEncoderWithMultiTask(num_classes_list)\n",
    "input_tensor = torch.randn(8, 3, 224, 224)\n",
    "outputs = model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f01ebf",
   "metadata": {},
   "source": [
    "## 测试pvt2模型导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451b78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"F:/AIE/test/PVT/classification/\")\n",
    "\n",
    "from pvt import pvt_medium  # Assuming the PVTv2 model is implemented in a file named 'pvt.py' within the repository\n",
    "\n",
    "model = pvt_medium(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9513f5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyramidVisionTransformer(\n",
       "  (patch_embed1): PatchEmbed(\n",
       "    (proj): Conv2d(3, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop1): Dropout(p=0.0, inplace=False)\n",
       "  (block1): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (kv): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (patch_embed2): PatchEmbed(\n",
       "    (proj): Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop2): Dropout(p=0.0, inplace=False)\n",
       "  (block2): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (patch_embed3): PatchEmbed(\n",
       "    (proj): Conv2d(128, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop3): Dropout(p=0.0, inplace=False)\n",
       "  (block3): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (12): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (13): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (14): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (15): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (16): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (17): Block(\n",
       "      (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (kv): Linear(in_features=320, out_features=640, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (patch_embed4): PatchEmbed(\n",
       "    (proj): Conv2d(320, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop4): Dropout(p=0.0, inplace=False)\n",
       "  (block4): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate=none)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fce7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
